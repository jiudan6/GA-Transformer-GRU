{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81dea77e4a83ebb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T15:41:48.508327Z",
     "start_time": "2025-06-05T15:41:48.455656Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "torch.cuda.empty_cache()\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, confusion_matrix\n",
    "# 设置是否使用GPU加速\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import random\n",
    "import pickle\n",
    "import os\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bf63cf5618cb9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T15:41:49.975973Z",
     "start_time": "2025-06-05T15:41:49.800093Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_data(file_paths):\n",
    "    dataframes = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_csv(file_path)\n",
    "        dataframes.append(df)\n",
    "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "    return combined_df\n",
    "\n",
    "\n",
    "file_paths = [\n",
    "    'D://数据//溢流数据交付//中值滤波//中值滤波结果18.csv',\n",
    "    'D://数据//溢流数据交付//中值滤波//中值滤波结果17.csv',\n",
    "    'D://数据//溢流数据交付//中值滤波//中值滤波结果25.csv',\n",
    "    'D://数据//溢流数据交付//中值滤波//中值滤波结果10.csv',\n",
    "    'D://数据//溢流数据交付//中值滤波//中值滤波结果16.csv',\n",
    "    'D://数据//溢流数据交付//中值滤波//中值滤波结果6.csv',\n",
    "    \n",
    "]\n",
    "\n",
    "combined_df = load_data(file_paths)\n",
    "\n",
    "\n",
    "test_df_26 = pd.read_csv('D://数据//溢流数据交付//中值滤波//中值滤波结果27.csv')\n",
    "\n",
    "\n",
    "def process_data(df):\n",
    "    feature_columns =  ['出口密度', '总池体积', '入口密度', '立管压力', '出口流量(百分)', '大钩负荷', '钻压', '出口温度', '扭矩', '转盘转速']\n",
    "    x = df[feature_columns].values\n",
    "    y = df['溢流'].values\n",
    "    return x, y\n",
    "\n",
    "\n",
    "x_train, y_train = process_data(combined_df)\n",
    "\n",
    "\n",
    "x_test, y_test = process_data(test_df_26)\n",
    "\n",
    "\n",
    "def augment_data_with_borderline_smote(x, y):\n",
    "    smote = BorderlineSMOTE(sampling_strategy=1)\n",
    "    x_resampled, y_resampled = smote.fit_resample(x, y)\n",
    "    return x_resampled, y_resampled\n",
    "\n",
    "\n",
    "x_train_combined, y_train_combined = augment_data_with_borderline_smote(x_train, y_train)\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "x_train_normalized = scaler.fit_transform(x_train_combined)\n",
    "x_test_normalized = scaler.transform(x_test)\n",
    "\n",
    "\n",
    "pca = PCA(n_components=6)\n",
    "x_train_pca = pca.fit_transform(x_train_normalized)\n",
    "x_test_pca = pca.transform(x_test_normalized)\n",
    "\n",
    "\n",
    "def save_preprocessor(preprocessors, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(preprocessors, f)\n",
    "\n",
    "\n",
    "preprocessors = {\n",
    "    'scaler': scaler,\n",
    "    'pca': pca\n",
    "}\n",
    "save_preprocessor(preprocessors, 'preprocessor.pkl')\n",
    "\n",
    "def print_class_distribution(y, dataset_name):\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    distribution = dict(zip(unique, counts))\n",
    "    print(f\"{dataset_name} 类别分布:\")\n",
    "    for label, count in distribution.items():\n",
    "        print(f\"类别 {label}: {count} 个样本\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442a26087e0c4dbd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T15:41:51.394497Z",
     "start_time": "2025-06-05T15:41:51.385499Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "param_ranges = [\n",
    "    (1, 5), \n",
    "    (16, 128),  \n",
    "    (0.00001, 0.05),  \n",
    "    (20, 500)  \n",
    "]\n",
    "\n",
    "\n",
    "pop_size =20  \n",
    "max_gru_layers = 5\n",
    "N_GENERATIONS = 20\n",
    "mutation_rate = 0.05\n",
    "crossover_rate = 0.6\n",
    "sequence_length = 540\n",
    "input_size = 10\n",
    "step_size=1\n",
    "dropout_rate=0.5\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cc1c695e278c29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T15:41:52.381352Z",
     "start_time": "2025-06-05T15:41:52.348353Z"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, input_size, num_heads, num_layers):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=input_size, nhead=num_heads, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.transformer_encoder(x)\n",
    "\n",
    "class CustomGRUModel(nn.Module):\n",
    "    def __init__(self, input_size, gru_units, transformer_layers, num_heads, dropout_rate=0.5):\n",
    "        super(CustomGRUModel, self).__init__()\n",
    "        self.transformer = TransformerEncoder(input_size=input_size, num_heads=num_heads, num_layers=transformer_layers)\n",
    "        self.grus = nn.ModuleList()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        for i in range(len(gru_units)):\n",
    "            input_dim = input_size if i == 0 else gru_units[i - 1]\n",
    "            self.grus.append(nn.GRU(input_size=input_dim, hidden_size=gru_units[i], num_layers=1, batch_first=True))\n",
    "        self.fc = nn.Linear(gru_units[-1], 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.transformer(x)\n",
    "        for gru in self.grus:\n",
    "            out, _ = gru(out)\n",
    "            out = self.dropout(out)\n",
    "        return torch.sigmoid(self.fc(out[:, -1, :]))\n",
    "\n",
    "def create_sliding_window(data, labels, sequence_length):\n",
    "    x_windows = []\n",
    "    y_windows = []\n",
    "\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        window = data[i:i + sequence_length]\n",
    "        x_windows.append(window)\n",
    "\n",
    "        if labels is not None:\n",
    "            future_label = max(labels[i + sequence_length:i + sequence_length + 1])\n",
    "            y_windows.append(future_label)\n",
    "\n",
    "    \n",
    "    if labels is not None:\n",
    "        return np.array(x_windows), np.array(y_windows)\n",
    "    else:\n",
    "        return np.array(x_windows), None  \n",
    "\n",
    "\n",
    "def log_metrics(log_file, message):\n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(message + '\\n')\n",
    "\n",
    "def aim_function(x_train_combined, y_train_combined, x_test, y_test, sequence_length, num, n_splits=2):\n",
    "    gru_layers = int(num[0])\n",
    "    gru_units = [int(num[i + 1]) for i in range(gru_layers)]\n",
    "    learning_rate = float(num[1 + gru_layers])\n",
    "    num_epochs = int(num[2 + gru_layers])\n",
    "\n",
    "    x_train_windows, y_train_windows = create_sliding_window(x_train_combined, y_train_combined, sequence_length)\n",
    "    x_test_windows, y_test_windows = create_sliding_window(x_test, y_test, sequence_length)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    fold_results = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(tscv.split(x_train_windows)):\n",
    "        print(f'Fold {fold + 1}')\n",
    "\n",
    "        train_dataset = TensorDataset(torch.tensor(x_train_windows[train_idx], dtype=torch.float32),\n",
    "                                      torch.tensor(y_train_windows[train_idx], dtype=torch.float32))\n",
    "        val_dataset = TensorDataset(torch.tensor(x_train_windows[val_idx], dtype=torch.float32),\n",
    "                                    torch.tensor(y_train_windows[val_idx], dtype=torch.float32))\n",
    "\n",
    "        batch_size = 256\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        model = CustomGRUModel(input_size=x_train_windows.shape[2], gru_units=gru_units, transformer_layers=2, num_heads=2).to(device)\n",
    "        optimizer = optim.Adamax(model.parameters(), lr=learning_rate)\n",
    "        criterion = nn.BCELoss()\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        patience = 5\n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            running_train_loss = 0.0\n",
    "            for batch_x, batch_y in train_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device).float().unsqueeze(1)\n",
    "                optimizer.zero_grad()\n",
    "                output = model(batch_x)\n",
    "                loss = criterion(output, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_train_loss += loss.item() * batch_x.size(0)\n",
    "\n",
    "            avg_train_loss = running_train_loss / len(train_loader.dataset)\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs}, Training Loss: {avg_train_loss:.4f}')\n",
    "\n",
    "            with torch.no_grad():\n",
    "                val_loss = evaluate(val_loader, model, 0.43 , device, criterion)[-1]\n",
    "                print(f'Epoch {epoch + 1}/{num_epochs}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch + 1}')\n",
    "                break\n",
    "\n",
    "        val_accuracy, val_f1, val_recall, _, _, _, _, val_loss = evaluate(val_loader, model, 0.43, device, criterion)\n",
    "        print(f'Validation Results for fold {fold + 1}: Accuracy={val_accuracy:.4f}, F1-score={val_f1:.4f}, Recall={val_recall:.4f}, Loss={val_loss:.4f}')\n",
    "        fold_results.append((val_accuracy, val_f1, val_recall))\n",
    "\n",
    "    avg_val_accuracy = np.mean([result[0] for result in fold_results])\n",
    "    avg_val_f1 = np.mean([result[1] for result in fold_results])\n",
    "    avg_val_recall = np.mean([result[2] for result in fold_results])\n",
    " \n",
    "    print(f'Average Validation Accuracy: {avg_val_accuracy:.4f}')\n",
    "    print(f'Average Validation F1-score: {avg_val_f1:.4f}')\n",
    "    print(f'Average Validation Recall: {avg_val_recall:.4f}')\n",
    "\n",
    "    test_dataset = TensorDataset(torch.tensor(x_test_windows, dtype=torch.float32), torch.tensor(y_test_windows, dtype=torch.float32))\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    test_accuracy, test_f1, test_recall, tn_test, fp_test, fn_test, tp_test, test_loss = evaluate(\n",
    "        test_loader, model, 0.61, device, criterion, save_probs=True, filename=\"predicted_probs.txt\"\n",
    "    )\n",
    "    test_message = (f'Final Test Accuracy: {test_accuracy:.4f}, Test F1-score: {test_f1:.4f}, '\n",
    "                    f'Test Recall: {test_recall:.4f}, Final Test Loss: {test_loss:.4f}')\n",
    "    print(test_message)\n",
    "    log_metrics(\"training_log.txt\", test_message)\n",
    "    matrix_message = f'Test Confusion Matrix: TN={tn_test}, FP={fp_test}, FN={fn_test}, TP={tp_test}'\n",
    "    print(matrix_message)\n",
    "    log_metrics(\"training_log.txt\", matrix_message)\n",
    "\n",
    "    return test_accuracy\n",
    "\n",
    "def evaluate(test_loader, model, threshold, device, criterion, save_probs=False, filename=\"predicted_probs.txt\"):\n",
    "    model.eval()\n",
    "    all_outputs = []\n",
    "    all_labels = []\n",
    "    running_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            output = model(batch_x)\n",
    "            all_outputs.append(output.cpu().numpy())\n",
    "            all_labels.append(batch_y.cpu().numpy())\n",
    "\n",
    "            batch_y = batch_y.float().unsqueeze(1)\n",
    "            loss = criterion(output, batch_y)\n",
    "            running_loss += loss.item() * batch_x.size(0)\n",
    "\n",
    "    avg_loss = running_loss / len(test_loader.dataset)\n",
    "\n",
    "    all_outputs = np.concatenate(all_outputs)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "\n",
    "    probs = torch.sigmoid(torch.tensor(all_outputs)).numpy()\n",
    "    preds = (probs >= threshold).astype(int)\n",
    "\n",
    "    if save_probs:\n",
    "        if os.path.exists(filename):\n",
    "            existing_probs = np.loadtxt(filename)\n",
    "            if existing_probs.ndim == 1:\n",
    "                existing_probs = existing_probs.reshape(-1, 1)\n",
    "            if probs.ndim == 1:\n",
    "                probs = probs.reshape(-1, 1)\n",
    "            if existing_probs.shape[1] != probs.shape[1]:\n",
    "                print(\"Warning: Shape mismatch between existing and new probabilities. Skipping appending.\")\n",
    "                all_probs = existing_probs  # 使用现有概率\n",
    "            else:\n",
    "                all_probs = np.vstack((existing_probs, probs))\n",
    "        else:\n",
    "            if probs.ndim == 1:\n",
    "                probs = probs.reshape(-1, 1)\n",
    "            all_probs = probs  # 设置初始 all_probs\n",
    "\n",
    "        # 确保在 save_probs 为 True 时，all_probs 一定被初始化\n",
    "        np.savetxt(filename, all_probs, fmt='%.6f')\n",
    "        print(f\"Predicted probabilities saved to {filename}\")\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, preds)\n",
    "    recall = recall_score(all_labels, preds)\n",
    "    f1 = f1_score(all_labels, preds)\n",
    "    tn, fp, fn, tp = confusion_matrix(all_labels, preds, labels=[0, 1]).ravel()\n",
    "\n",
    "    return accuracy, f1, recall, tn, fp, fn, tp, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a649aa41182a88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T15:41:53.633077Z",
     "start_time": "2025-06-05T15:41:53.621076Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_individual(param_ranges, max_gru_layers):\n",
    "    gru_layers = random.randint(1, max_gru_layers)  \n",
    "    gru_units = [random.randint(param_ranges[1][0], param_ranges[1][1]) for _ in range(gru_layers)] \n",
    "    learning_rate = random.uniform(param_ranges[2][0], param_ranges[2][1]) \n",
    "    num_epochs = random.randint(param_ranges[3][0], param_ranges[3][1])  \n",
    "    individual = [gru_layers] + gru_units + [learning_rate, num_epochs]\n",
    "    return individual\n",
    "\n",
    "\n",
    "def init_population(pop_size, max_gru_layers, param_ranges):\n",
    "    return [generate_individual(param_ranges, max_gru_layers) for _ in range(pop_size)]\n",
    "\n",
    "\n",
    "def encode_params(num):\n",
    "    binary_encoding = format(num[0], '03b')  \n",
    "    for i in range(1, 1 + num[0]):  \n",
    "        binary_encoding += format(num[i], '08b')\n",
    "    binary_encoding += format(int(num[1 + num[0]] * 1000), '011b') \n",
    "    binary_encoding += format(num[-1], '09b')  \n",
    "    return binary_encoding\n",
    "\n",
    "\n",
    "def decode_params(binary_encoding):\n",
    "    decoded = [int(binary_encoding[:3], 2)]  \n",
    "    for i in range(decoded[0]):\n",
    "        start = 3 + i * 8\n",
    "        decoded.append(int(binary_encoding[start:start + 8], 2))  \n",
    "\n",
    "    lr_value = int(binary_encoding[3 + decoded[0] * 8:3 + decoded[0] * 8 + 11], 2) / 100000.0\n",
    "    decoded_learning_rate = 0.00001 + lr_value * (0.0001 - 0.00001)\n",
    "    decoded.append(decoded_learning_rate) \n",
    "    decoded.append(int(binary_encoding[-9:], 2)) \n",
    "    return decoded\n",
    "\n",
    "\n",
    "def select(population, fitness):\n",
    "    fitness = np.where(fitness == 0, 1e-6, fitness)  \n",
    "    fitness = np.where(np.isnan(fitness), 1e-6, fitness)  \n",
    "    cumulative_prob = np.cumsum(fitness / np.sum(fitness))\n",
    "    r1, r2 = np.random.rand(2)\n",
    "\n",
    "    parent1, parent2 = None, None\n",
    "    for i, cp in enumerate(cumulative_prob):\n",
    "        if parent1 is None and r1 < cp:\n",
    "            parent1 = population[i]\n",
    "        if parent2 is None and r2 < cp and parent1 is not population[i]:\n",
    "            parent2 = population[i]\n",
    "        if parent1 is not None and parent2 is not None:\n",
    "            break\n",
    "\n",
    "    if parent1 is None or parent2 is None:\n",
    "        available_indices = list(range(len(population)))\n",
    "        if parent1 is None:\n",
    "            parent1 = population[random.choice(available_indices)]\n",
    "        if parent2 is None:\n",
    "            available_indices.remove(population.index(parent1))\n",
    "            parent2 = population[random.choice(available_indices)]\n",
    "\n",
    "    return parent1, parent2\n",
    "\n",
    "\n",
    "def uniform_crossover(parent1, parent2, crossover_rate):\n",
    "    if parent1 is None or parent2 is None:\n",
    "        raise ValueError(\"Parents cannot be None\")\n",
    "\n",
    "    child = ''\n",
    "    for i in range(len(parent1)):\n",
    "        if np.random.rand() < crossover_rate:\n",
    "            child += parent2[i]\n",
    "        else:\n",
    "            child += parent1[i]\n",
    "\n",
    "    return child\n",
    "\n",
    "\n",
    "def mutate(dna, mutation_rate):\n",
    "    mutated = ''\n",
    "    for bit in dna:\n",
    "        if np.random.rand() < mutation_rate:\n",
    "            mutated += '1' if bit == '0' else '0'\n",
    "        else:\n",
    "            mutated += bit\n",
    "    return mutated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5039d0f5cf72f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T15:42:12.989033600Z",
     "start_time": "2025-06-05T15:41:54.507955Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def save_model(model, file_path):\n",
    "    torch.save(model.state_dict(), file_path)\n",
    "\n",
    "best_model_params = None\n",
    "population = [encode_params(dna) for dna in init_population(pop_size, max_gru_layers, param_ranges)]\n",
    "best_solution = None\n",
    "best_fitness = -np.inf\n",
    "\n",
    "for generation in range(N_GENERATIONS):\n",
    "    fitness = np.zeros(pop_size)\n",
    "    for i, dna in enumerate(population):\n",
    "        params = decode_params(dna)\n",
    "        print(f\"Evaluating individual {i} with params: {params}\")\n",
    "        weighted_score = aim_function(x_train_combined, y_train_combined, x_test, y_test, sequence_length, num=params)\n",
    "        fitness[i] = weighted_score\n",
    "        print(f\"Fitness[{i}] = {fitness[i]}\")\n",
    "       \n",
    "\n",
    "    best_idx = np.argmax(fitness)\n",
    "    if fitness[best_idx] > best_fitness:\n",
    "        best_fitness = fitness[best_idx]\n",
    "        best_solution = decode_params(population[best_idx])\n",
    "        best_model_params = best_solution  \n",
    "\n",
    "    print(f'Generation {generation + 1}, Best Fitness: {best_fitness:.4f}, Best Params: {best_solution}')\n",
    "\n",
    "    # 选择父代进行交叉和变异\n",
    "    new_population = []\n",
    "    for _ in range(pop_size // 2):\n",
    "        parent1, parent2 = select(population, fitness)\n",
    "        child1_dna = uniform_crossover(parent1, parent2, crossover_rate)\n",
    "        child2_dna = uniform_crossover(parent1, parent2, crossover_rate)\n",
    "        new_population.append(mutate(child1_dna, mutation_rate))\n",
    "        new_population.append(mutate(child2_dna, mutation_rate))\n",
    "\n",
    "    population = new_population\n",
    "\n",
    "if best_model_params is not None:\n",
    "    print(f'Training final model with best parameters: {best_model_params}')\n",
    "    gru_layers = int(best_model_params[0])\n",
    "    gru_units = [int(best_model_params[i + 1]) for i in range(gru_layers)]\n",
    "    learning_rate = float(best_model_params[1 + gru_layers])\n",
    "    num_epochs = int(best_model_params[2 + gru_layers])\n",
    "\n",
    "    model = CustomGRUModel(input_size=x_train_combined.shape[1], gru_units=gru_units, transformer_layers=2, num_heads=2).to(device)\n",
    "    optimizer = optim.Adamax(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    x_train_windows, y_train_windows = create_sliding_window(x_train_combined, y_train_combined, sequence_length)\n",
    "    \n",
    "    train_dataset = TensorDataset(torch.tensor(x_train_windows, dtype=torch.float32),\n",
    "                                  torch.tensor(y_train_windows, dtype=torch.float32))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device).float().unsqueeze(1)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_x)\n",
    "            loss = criterion(output, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    \n",
    "model_params = {\n",
    "    'input_size': x_train_combined.shape[1],\n",
    "    'gru_units': gru_units,  \n",
    "    'transformer_layers': 2,  \n",
    "    'num_heads': 2  \n",
    "}\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_params': model_params\n",
    "}, 'best_gru_model.pth')\n",
    "\n",
    "# 输出最佳结果\n",
    "print(f'Best Parameters: {best_solution}')\n",
    "print(f'Best Fitness Score: {best_fitness:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dadc2e37dd5a51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T15:41:33.702956700Z",
     "start_time": "2024-11-02T08:35:03.813823Z"
    }
   },
   "outputs": [],
   "source": [
    "# 加载预处理参数\n",
    "def load_preprocessor(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        preprocessors = pickle.load(f)\n",
    "    return preprocessors\n",
    "\n",
    "preprocessors = load_preprocessor('preprocessor.pkl')\n",
    "scaler = preprocessors['scaler']\n",
    "pca = preprocessors['pca']\n",
    "\n",
    "def load_model(file_path, device):\n",
    "    checkpoint = torch.load(file_path)\n",
    "    model_params = checkpoint['model_params']\n",
    "    model = CustomGRUModel(\n",
    "        input_size=model_params['input_size'],\n",
    "        gru_units=model_params['gru_units'],\n",
    "        transformer_layers=model_params['transformer_layers'],\n",
    "        num_heads=model_params['num_heads']\n",
    "    )\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "best_model = load_model('best_gru_model.pth', device)\n",
    "def preprocess_data(df, scaler, pca):\n",
    "    feature_columns =  ['出口密度', '总池体积', '入口密度', '立管压力', '出口流量(百分)', '大钩负荷', '钻压', '出口温度', '扭矩', '转盘转速']\n",
    "    x = df[feature_columns].values\n",
    "    x_normalized = scaler.transform(x)\n",
    "    x_pca = pca.transform(x_normalized)\n",
    "    return x_pca\n",
    "\n",
    "# 加载并处理新数据\n",
    "data_path = \"D://数据//溢流数据交付//中值滤波//中值滤波结果4.csv\"\n",
    "new_data = pd.read_csv(data_path)\n",
    "processed_data = preprocess_data(new_data, scaler, pca)\n",
    "\n",
    "true_labels = new_data['溢流'].values  \n",
    "\n",
    "def create_sliding_window(data, labels, sequence_length, stride=1):\n",
    "    x_windows = []\n",
    "    y_windows = []\n",
    "    for i in range(0, len(data) - sequence_length + 1, stride):\n",
    "        x_windows.append(data[i:i + sequence_length])\n",
    "        if labels is not None:\n",
    "            y_windows.append(labels[i + sequence_length - 1])\n",
    "    return np.array(x_windows), (np.array(y_windows) if labels is not None else None)\n",
    "\n",
    "# 创建滑动窗口的数据\n",
    "sequence_length = 540\n",
    "x_windows, y_windows = create_sliding_window(processed_data, true_labels, sequence_length)\n",
    "\n",
    "x_windows_tensor = torch.tensor(x_windows, dtype=torch.float32).to(device)\n",
    "\n",
    "best_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = best_model(x_windows_tensor)\n",
    "\n",
    "probabilities = torch.sigmoid(predictions).cpu().numpy()\n",
    "\n",
    "threshold = 0.3\n",
    "predicted_classes = (probabilities >= threshold).astype(int).flatten()\n",
    "\n",
    "# 打印预测概率\n",
    "print(\"Predicted probabilities:\\n\", probabilities)\n",
    "\n",
    "# 确保长度匹配\n",
    "print(f\"Length of y_windows: {len(y_windows)}\")\n",
    "print(f\"Length of predicted_classes: {len(predicted_classes)}\")\n",
    "\n",
    "# 将预测概率保存到 CSV 文件\n",
    "probabilities_df = pd.DataFrame(probabilities, columns=['Probability'])  # 可以调整列名\n",
    "probabilities_df.to_csv('predicted_probabilities.csv', index=False)\n",
    "\n",
    "# 计算准确率\n",
    "accuracy = accuracy_score(y_windows, predicted_classes)\n",
    "\n",
    "# 计算召回率\n",
    "recall = recall_score(y_windows, predicted_classes)\n",
    "\n",
    "# 计算F1分数\n",
    "f1 = f1_score(y_windows, predicted_classes)\n",
    "\n",
    "# 计算混淆矩阵\n",
    "conf_matrix = confusion_matrix(y_windows, predicted_classes)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
